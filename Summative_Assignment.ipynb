{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ell-sie/alu-machine_learning/blob/main/Summative_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAt-K2qgcIou"
      },
      "source": [
        "# Optimization Using Gradient Descent: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZYK-0rin5x7"
      },
      "source": [
        "In this assignment, you will build a simple linear regression model to predict sales based on TV marketing expenses. You will investigate three different approaches to this problem. You will use `NumPy` and `Scikit-Learn` linear regression models, as well as construct and optimize the sum of squares cost function with gradient descent from scratch.\n",
        "\n",
        "Further you will add additional cells to compare Linear regression and atleast 1 other algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKmRzv7Yn9gp"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [ 1 - Open the Dataset and State the Problem]\n",
        "  - [ Exercise 1]\n",
        "- [ 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`]\n",
        "  - [ 2.1 - Linear Regression with `NumPy`]\n",
        "    - [ Exercise 2]\n",
        "  - [ 2.2 - Linear Regression with `Scikit-Learn`]\n",
        "    - [ Exercise 3]\n",
        "    - [ Exercise 4]\n",
        "- [ 3 - Linear Regression using Gradient Descent]\n",
        "  - [ Exercise 5]\n",
        "  - [ Exercise 6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9vh1alen9gq"
      },
      "source": [
        "## Packages\n",
        "\n",
        "Load the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "grSlR3zzn9gr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# A library for programmatic plot generation.\n",
        "\n",
        "# A library for data manipulation and analysis.\n",
        "\n",
        "# LinearRegression from sklearn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEpybYHin9gv"
      },
      "source": [
        "Import the unit tests defined for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9MSur1mn9gw"
      },
      "outputs": [],
      "source": [
        "import w2_unittest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvPP9MKGn9gx"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Open the Dataset and State the Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMKBkrOyn9gy"
      },
      "source": [
        "In this lab, you will build a linear regression model for a simple Kaggle dataset, saved in a file `data/tvmarketing.csv`. The dataset has only two fields: TV marketing expenses (`TV`) and sales amount (`Sales`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZpXhJazn9gy"
      },
      "source": [
        "<a name='ex01'></a>\n",
        "### Exercise 1\n",
        "\n",
        "Use `pandas` function `pd.read_csv` to open the .csv file the from the `path`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "CQ6KKECzn9gz"
      },
      "outputs": [],
      "source": [
        "path = \"data/tvmarketing.csv\"\n",
        "\n",
        "### START CODE HERE ### (~ 1 line of code)\n",
        "adv = #None.None(None)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "AoJ2pGdRn9gz"
      },
      "outputs": [],
      "source": [
        "# Print some part of the dataset.\n",
        "adv.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZcoL00Zn9gz"
      },
      "source": [
        "##### __Expected Output__\n",
        "\n",
        "```Python\n",
        "\tTV\tSales\n",
        "0\t230.1\t22.1\n",
        "1\t44.5\t10.4\n",
        "2\t17.2\t9.3\n",
        "3\t151.5\t18.5\n",
        "4\t180.8\t12.9\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwOpM2Yon9g0"
      },
      "outputs": [],
      "source": [
        "w2_unittest.test_load_data(adv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8XUy_Bun9g0"
      },
      "source": [
        "`pandas` has a function to make plots from the DataFrame fields. By default, matplotlib is used at the backend. Let's use it here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "gFnyoSvhn9g0"
      },
      "outputs": [],
      "source": [
        "adv.plot(x='TV', y='Sales', kind='scatter', c='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQqLgTkfn9g0"
      },
      "source": [
        "You can use this dataset to solve a simple problem with linear regression: given a TV marketing budget, predict sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01-QHzSgn9g0"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Linear Regression in Python with `NumPy` and `Scikit-Learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSaEw7xn9g1"
      },
      "source": [
        "Save the required field of the DataFrame into variables `X` and `Y`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "-B90FpBCn9g1"
      },
      "outputs": [],
      "source": [
        "X = adv['TV']\n",
        "Y = adv['Sales']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWyvC6jen9g1"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 - Linear Regression with `NumPy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_tCzATwn9g1"
      },
      "source": [
        "You can use the function `np.polyfit(x, y, deg)` to fit a polynomial of degree `deg` to points $(x, y)$, minimising the sum of squared errors. You can read more in the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). Taking `deg = 1` you can obtain the slope `m` and the intercept `b` of the linear regression line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "HsCOc6FVn9g1"
      },
      "outputs": [],
      "source": [
        "m_numpy, b_numpy = # insert code here\n",
        "\n",
        "print(f\"Linear regression with NumPy. Slope: {m_numpy}. Intercept: {b_numpy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwWs2Lw5n9g1"
      },
      "source": [
        "*Note*: [`NumPy` documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html) suggests the [`Polynomial.fit` class method](https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit) as recommended for new code as it is more stable numerically. But in this simple example, you can stick to the `np.polyfit` function for simplicity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yzlLxn7n9g2"
      },
      "source": [
        "<a name='ex02'></a>\n",
        "### Exercise 2\n",
        "\n",
        "Make predictions substituting the obtained slope and intercept coefficients into the equation $Y = mX + b$, given an array of $X$ values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "YDd0JG6Fn9g2"
      },
      "outputs": [],
      "source": [
        "# This is organised as a function only for grading purposes.\n",
        "def pred_numpy(m, b, X):\n",
        "\n",
        "    # TO DO: Insert code here\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "HWIJSKQln9g2"
      },
      "outputs": [],
      "source": [
        "X_pred = np.array([50, 120, 280])\n",
        "Y_pred_numpy = pred_numpy(m_numpy, b_numpy, X_pred)\n",
        "\n",
        "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
        "print(f\"Predictions of sales using NumPy linear regression:\\n{Y_pred_numpy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9owUPj2xn9g2"
      },
      "source": [
        "##### __Expected Output__\n",
        "\n",
        "```Python\n",
        "TV marketing expenses:\n",
        "[ 50 120 280]\n",
        "Predictions of sales using NumPy linear regression:\n",
        "[ 9.40942557 12.7369904  20.34285287]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy0gZt_xn9g2"
      },
      "outputs": [],
      "source": [
        "w2_unittest.test_pred_numpy(pred_numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbTp-x4Fn9g3"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### 2.2 - Linear Regression with `Scikit-Learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qsJ5M2fn9g3"
      },
      "source": [
        "`Scikit-Learn` is an open-source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection, model evaluation, and many other utilities. `Scikit-learn` provides dozens of built-in machine learning algorithms and models, called **estimators**. Each estimator can be fitted to some data using its `fit` method. Full documentation can be found [here](https://scikit-learn.org/stable/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4PjAc_2n9g3"
      },
      "source": [
        "Create an estimator object for a linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wN6WDXZdn9g3"
      },
      "outputs": [],
      "source": [
        "lr_sklearn = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ1SpMayn9g3"
      },
      "source": [
        "The estimator can learn from data calling the `fit` function. However, trying to run the following code you will get an error, as the data needs to be reshaped into 2D array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9DzJamRyn9g3"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of X array: {X.shape}\")\n",
        "print(f\"Shape of Y array: {Y.shape}\")\n",
        "\n",
        "try:\n",
        "    lr_sklearn.fit(X, Y)\n",
        "except ValueError as err:\n",
        "    print(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU2uCNu1n9g3"
      },
      "source": [
        "You can increase the dimension of the array by one with `reshape` function, or there is another another way to do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "DduKQisWn9g4"
      },
      "outputs": [],
      "source": [
        "X_sklearn = X[:, np.newaxis]\n",
        "Y_sklearn = Y[:, np.newaxis]\n",
        "\n",
        "print(f\"Shape of new X array: {X_sklearn.shape}\")\n",
        "print(f\"Shape of new Y array: {Y_sklearn.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RWziUH6n9g4"
      },
      "source": [
        "You have already loaded your dataset into X_sklearn and Y_sklearn\n",
        "Step 1: Split the data into training and testing sets use train_test_split from sklearn\n",
        "The test size shoukd be 20% of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T-6MF6Cn9g4"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, Y_train, Y_test = #Insert Code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkz3m8_n9hG"
      },
      "source": [
        "Step 2: Fit the linear regression model to the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeyRFmh-n9hG"
      },
      "outputs": [],
      "source": [
        "lr_sklearn.fit() #Insert proper arguments fro training asper step 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7V-n-h0n9hG"
      },
      "source": [
        "\n",
        " Step 3: Make predictions using the fitted model on the testing data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW0663B5n9hG"
      },
      "outputs": [],
      "source": [
        "Y_pred = model.predict( None)#use test data from X from step 1 above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BfnClVRn9hG"
      },
      "source": [
        " Step 4: Calculate the RMSE\n",
        "Using sklearn.metrics - mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axiMzGp6n9hG"
      },
      "outputs": [],
      "source": [
        "#Insert your code here\n",
        "rmse =  # TO DO\n",
        "print(\"Root Mean Square Error:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrmTKSlRn9hH"
      },
      "source": [
        "TO DO Create an estimator object for Random Forest and Desision Trees and compare RSMES:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6BU7Byan9hH",
        "outputId": "8c3880ce-d145-4bcc-9f1d-d750229ea1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'linear regression': '0.1', 'Random Forest': '0.1', 'Decision Trees': '0.1'}\n"
          ]
        }
      ],
      "source": [
        "# To do\n",
        "#Print out the rank of models From the best to the worst performing and associated RSMEs\n",
        "\n",
        "#Replace the code below with appropriatly\n",
        "\n",
        "model_rank = {\n",
        "'linear regression' : '0.1',\n",
        "'Random Forest' : '0.1',\n",
        "'Decision Trees' : '0.1'\n",
        "\n",
        "\n",
        "}\n",
        "print (model_rank)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcdRSq30n9hH"
      },
      "source": [
        "The estimator can learn from data calling the `fit` function for RandomForest and Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKS4tdB7n9hH"
      },
      "source": [
        "Compare the RSME for the three different models and rank them according to performance i.e Print out Model Rank and Associated RSME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXpTRi0Nn9hI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lidgkss_n9hI"
      },
      "source": [
        "<a name='ex03'></a>\n",
        "### Exercise 3\n",
        "\n",
        "Fit the linear regression model passing `X_sklearn` and `Y_sklearn` arrays into the function `lr_sklearn.fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "8day7zZDn9hI"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (~ 1 line of code)\n",
        "None.None(None, None)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "hOks7a4zn9hI"
      },
      "outputs": [],
      "source": [
        "m_sklearn = lr_sklearn.coef_\n",
        "b_sklearn = lr_sklearn.intercept_\n",
        "\n",
        "print(f\"Linear regression using Scikit-Learn. Slope: {m_sklearn}. Intercept: {b_sklearn}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdAjROopn9hI"
      },
      "source": [
        "##### __Expected Output__\n",
        "\n",
        "```Python\n",
        "Linear regression using Scikit-Learn. Slope: [[0.04753664]]. Intercept: [7.03259355]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNzx3pw0n9hI"
      },
      "outputs": [],
      "source": [
        "w2_unittest.test_sklearn_fit(lr_sklearn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyFNUm4on9hI"
      },
      "source": [
        "Note that you have got the same result as with the `NumPy` function `polyfit`. Now, to make predictions it is convenient to use `Scikit-Learn` function `predict`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5TrefQ9n9hI"
      },
      "source": [
        "<a name='ex04'></a>\n",
        "### Exercise 4\n",
        "\n",
        "\n",
        "Increase the dimension of the $X$ array using the function `np.newaxis` (see an example above) and pass the result to the `lr_sklearn.predict` function to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jADTqBSNn9hI"
      },
      "outputs": [],
      "source": [
        "# This is organised as a function only for grading purposes.\n",
        "def pred_sklearn(X, lr_sklearn):\n",
        "    ### START CODE HERE ### (~ 2 lines of code)\n",
        "    X_2D = None[None, None.None]\n",
        "    Y = None.None(None)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FNP8_KpKn9hJ",
        "outputId": "b08cdeab-4c92-4780-8260-36f8ac0097cc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/seer/Downloads/Optimization with multiple variables/C2_W2_Assignment (1).ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/seer/Downloads/Optimization%20with%20multiple%20variables/C2_W2_Assignment%20%281%29.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Y_pred_sklearn \u001b[39m=\u001b[39m pred_sklearn(X_pred, lr_sklearn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/seer/Downloads/Optimization%20with%20multiple%20variables/C2_W2_Assignment%20%281%29.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTV marketing expenses:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mX_pred\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/seer/Downloads/Optimization%20with%20multiple%20variables/C2_W2_Assignment%20%281%29.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions of sales using Scikit_Learn linear regression:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mY_pred_sklearn\u001b[39m.\u001b[39mT\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;32m/home/seer/Downloads/Optimization with multiple variables/C2_W2_Assignment (1).ipynb Cell 59\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/seer/Downloads/Optimization%20with%20multiple%20variables/C2_W2_Assignment%20%281%29.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Y_pred_sklearn \u001b[39m=\u001b[39m pred_sklearn(X_pred, lr_sklearn)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/seer/Downloads/Optimization%20with%20multiple%20variables/C2_W2_Assignment%20%281%29.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTV marketing expenses:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mX_pred\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/seer/Downloads/Optimization%20with%20multiple%20variables/C2_W2_Assignment%20%281%29.ipynb#X63sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions of sales using Scikit_Learn linear regression:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mY_pred_sklearn\u001b[39m.\u001b[39mT\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "Y_pred_sklearn = pred_sklearn(X_pred, lr_sklearn)\n",
        "\n",
        "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
        "print(f\"Predictions of sales using Scikit_Learn linear regression:\\n{Y_pred_sklearn.T}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSr8KVydn9hJ"
      },
      "source": [
        "##### __Expected Output__\n",
        "\n",
        "```Python\n",
        "TV marketing expenses:\n",
        "[ 50 120 280]\n",
        "Predictions of sales using Scikit_Learn linear regression:\n",
        "[[ 9.40942557 12.7369904  20.34285287]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR74Yk_Mn9hJ"
      },
      "outputs": [],
      "source": [
        "w2_unittest.test_sklearn_predict(pred_sklearn, lr_sklearn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snm7JrfDn9hJ"
      },
      "source": [
        "You can plot the linear regression line and the predictions by running the following code. The regression line is red and the predicted points are blue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "GW55zi6pn9hJ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,1,figsize=(8,5))\n",
        "ax.plot(X, Y, 'o', color='black')\n",
        "ax.set_xlabel('TV')\n",
        "ax.set_ylabel('Sales')\n",
        "\n",
        "ax.plot(X, m_sklearn[0][0]*X+b_sklearn[0], color='red')\n",
        "ax.plot(X_pred, Y_pred_sklearn, 'o', color='blue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ANzpYVin9hJ"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Linear Regression using Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QLUsk2Fn9hJ"
      },
      "source": [
        "Functions to fit the models automatically are convenient to use, but for an in-depth understanding of the model and the maths behind it is good to implement an algorithm by yourself. Let's try to find linear regression coefficients $m$ and $b$, by minimising the difference between original values $y^{(i)}$ and predicted values $\\hat{y}^{(i)}$ with the **loss function** $L\\left(w, b\\right)  = \\frac{1}{2}\\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$ for each of the training examples. Division by $2$ is taken just for scaling purposes, you will see the reason below, calculating partial derivatives.\n",
        "\n",
        "To compare the resulting vector of the predictions $\\hat{Y}$ with the vector $Y$ of original values $y^{(i)}$, you can take an average of the loss function values for each of the training examples:\n",
        "\n",
        "$$E\\left(m, b\\right) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 =\n",
        "\\frac{1}{2n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)^2,\\tag{1}$$\n",
        "\n",
        "where $n$ is a number of data points. This function is called the sum of squares **cost function**. To use gradient descent algorithm, calculate partial derivatives as:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial E }{ \\partial m } &=\n",
        "\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right)x^{(i)},\\\\\n",
        "\\frac{\\partial E }{ \\partial b } &=\n",
        "\\frac{1}{n}\\sum_{i=1}^{n} \\left(mx^{(i)}+b - y^{(i)}\\right),\n",
        "\\tag{2}\\end{align}\n",
        "\n",
        "and update the parameters iteratively using the expressions\n",
        "\n",
        "\\begin{align}\n",
        "m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n",
        "b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n",
        "\\tag{3}\\end{align}\n",
        "\n",
        "where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5ibcaGIn9hK"
      },
      "source": [
        "Original arrays `X` and `Y` have different units. To make gradient descent algorithm efficient, you need to bring them to the same units. A common approach to it is called **normalization**: substract the mean value of the array from each of the elements in the array and divide them by standard deviation (a statistical measure of the amount of dispersion of a set of values). If you are not familiar with mean and standard deviation, do not worry about this for now - this is covered in the next Course of Specialization.\n",
        "\n",
        "Normalization is not compulsory - gradient descent would work without it. But due to different units of `X` and `Y`, the cost function will be much steeper. Then you would need to take a significantly smaller learning rate $\\alpha$, and the algorithm will require thousands of iterations to converge instead of a few dozens. Normalization helps to increase the efficiency of the gradient descent algorithm.\n",
        "\n",
        "Normalization is implemented in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "YH3RnYrwn9hK"
      },
      "outputs": [],
      "source": [
        "X_norm = (X - np.mean(X))/np.std(X)\n",
        "Y_norm = (Y - np.mean(Y))/np.std(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW4e0VBZn9hK"
      },
      "source": [
        "Define cost function according to the equation $(1)$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "IyEfqJmEn9hK"
      },
      "outputs": [],
      "source": [
        "def E(m, b, X, Y):\n",
        "    return 1/(2*len(Y))*np.sum((m*X + b - Y)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVroFWftn9hK"
      },
      "source": [
        "<a name='ex05'></a>\n",
        "### Exercise 5\n",
        "\n",
        "\n",
        "Define functions `dEdm` and `dEdb` to calculate partial derivatives according to the equations $(2)$. This can be done using vector form of the input data `X` and `Y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "ArLlQzBln9hK"
      },
      "outputs": [],
      "source": [
        "def dEdm(m, b, X, Y):\n",
        "    ### START CODE HERE ### (~ 1 line of code)\n",
        "    # Use the following line as a hint, replacing all None.\n",
        "    res = 1/len(None)*np.dot(None*None + None - None, None)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def dEdb(m, b, X, Y):\n",
        "    ### START CODE HERE ### (~ 1 line of code)\n",
        "    # Replace None writing the required expression fully.\n",
        "    res = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "42HIGLpxn9hK"
      },
      "outputs": [],
      "source": [
        "print(dEdm(0, 0, X_norm, Y_norm))\n",
        "print(dEdb(0, 0, X_norm, Y_norm))\n",
        "print(dEdm(1, 5, X_norm, Y_norm))\n",
        "print(dEdb(1, 5, X_norm, Y_norm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26LoyLoxn9hK"
      },
      "source": [
        "##### __Expected Output__\n",
        "\n",
        "```Python\n",
        "-0.7822244248616067\n",
        "5.098005351200641e-16\n",
        "0.21777557513839355\n",
        "5.000000000000002\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI7GHMaZn9hK"
      },
      "outputs": [],
      "source": [
        "w2_unittest.test_partial_derivatives(dEdm, dEdb, X_norm, Y_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0v2VhcMn9hL"
      },
      "source": [
        "<a name='ex06'></a>\n",
        "### Exercise 6\n",
        "\n",
        "\n",
        "Implement gradient descent using expressions $(3)$:\n",
        "\\begin{align}\n",
        "m &= m - \\alpha \\frac{\\partial E }{ \\partial m },\\\\\n",
        "b &= b - \\alpha \\frac{\\partial E }{ \\partial b },\n",
        "\\end{align}\n",
        "\n",
        "where $\\alpha$ is the `learning_rate`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "_lKjyaMLn9hL"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(dEdm, dEdb, m, b, X, Y, learning_rate = 0.001, num_iterations = 1000, print_cost=False):\n",
        "    for iteration in range(num_iterations):\n",
        "        ### START CODE HERE ### (~ 2 lines of code)\n",
        "        m_new = None\n",
        "        b_new = None\n",
        "        ### END CODE HERE ###\n",
        "        m = m_new\n",
        "        b = b_new\n",
        "        if print_cost:\n",
        "            print (f\"Cost after iteration {iteration}: {E(m, b, X, Y)}\")\n",
        "\n",
        "    return m, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "5eUx5uhjn9hL"
      },
      "outputs": [],
      "source": [
        "print(gradient_descent(dEdm, dEdb, 0, 0, X_norm, Y_norm))\n",
        "print(gradient_descent(dEdm, dEdb, 1, 5, X_norm, Y_norm, learning_rate = 0.01, num_iterations = 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVVkpU2nn9hL"
      },
      "source": [
        "##### __Expected Output__\n",
        "\n",
        "```Python\n",
        "(0.49460408269589495, -3.489285249624889e-16)\n",
        "(0.9791767513915026, 4.521910375044022)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLUEz_pun9hL"
      },
      "outputs": [],
      "source": [
        "w2_unittest.test_gradient_descent(gradient_descent, dEdm, dEdb, X_norm, Y_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfgT8vcFn9hL"
      },
      "source": [
        "Now run the gradient descent method starting from the initial point $\\left(m_0, b_0\\right)=\\left(0, 0\\right)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "rHQqOHPgn9hL"
      },
      "outputs": [],
      "source": [
        "m_initial = 0; b_initial = 0; num_iterations = 30; learning_rate = 1.2\n",
        "m_gd, b_gd = gradient_descent(dEdm, dEdb, m_initial, b_initial,\n",
        "                              X_norm, Y_norm, learning_rate, num_iterations, print_cost=True)\n",
        "\n",
        "print(f\"Gradient descent result: m_min, b_min = {m_gd}, {b_gd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ee3gB04n9hL"
      },
      "source": [
        "Remember, that the initial datasets were normalized. To make the predictions, you need to normalize `X_pred` array, calculate `Y_pred` with the linear regression coefficients `m_gd`, `b_gd` and then **denormalize** the result (perform the reverse process of normalization):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ubKFwBKWn9hL"
      },
      "outputs": [],
      "source": [
        "X_pred = np.array([50, 120, 280])\n",
        "# Use the same mean and standard deviation of the original training array X\n",
        "X_pred_norm = (X_pred - np.mean(X))/np.std(X)\n",
        "Y_pred_gd_norm = m_gd * X_pred_norm + b_gd\n",
        "# Use the same mean and standard deviation of the original training array Y\n",
        "Y_pred_gd = Y_pred_gd_norm * np.std(Y) + np.mean(Y)\n",
        "\n",
        "print(f\"TV marketing expenses:\\n{X_pred}\")\n",
        "print(f\"Predictions of sales using Scikit_Learn linear regression:\\n{Y_pred_sklearn.T}\")\n",
        "print(f\"Predictions of sales using Gradient Descent:\\n{Y_pred_gd}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKRmcDNDn9hL"
      },
      "source": [
        "You should have gotten similar results as in the previous sections.\n",
        "\n",
        "Well done! Now you know how gradient descent algorithm can be applied to train a real model. Re-producing results manually for a simple case should give you extra confidence that you understand what happends under the hood of commonly used functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "graded"
        ],
        "id": "1NOM4JKZn9hM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "AI4MC1-1"
      ]
    },
    "grader_version": "1",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "478841ab876a4250505273c8a697bbc1b6b194054b009c227dc606f17fb56272"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}